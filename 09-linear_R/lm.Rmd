# Linear Regression

### `pace` example

This dataset represents pedestrian walking speed and population for
fifteen cities. Let's predict walking speed from population density.

```{r}
pace <- read.csv('pace.csv')
head(pace)
```

It's not uncommon to be handed a dataset like this with no
documentation. In the case of this example, you would be able to find
out a lot with some searching. Here's a little more context. The
cities are:

 * Brno, Czechoslovakia
 * Prague, Czechoslovakia
 * Corte, Corsica
 * Bastia, France
 * Munich, Germany
 * Psychro, Crete
 * Itea, Greece
 * Iraklion, Greece
 * Athens, Greece
 * Safed, Israel
 * Dimona, Israel
 * Netanya, Israel
 * Jerusalem, Israel
 * New Haven, U.S.A.
 * Brooklyn, U.S.A.

The units are feet per second for speed and count of population in
millions for population. The data comes originally from an article in
_Nature_, and has since been popularized by educators.

> Bornstein, Marc H., and Bornstein, Helen G. "The Pace of Life,"
  Nature 259 (19 February 1976): 557-559.

A scatterplot of the data shows an obviously nonlinear relationship.

```{r}
# Base R
plot(speed ~ pop, data=pace)
# ggplot2 with text
library(ggplot2)
ggplot(pace, aes(y=speed, x=pop, label=city)) + geom_text()
```

Fitting a linear model to this dataset produces significant
coefficients with an R-squared of ~43%, which is not bad. But based on
the shape of the data, we can probably do better.

```{r}
linear.fit <- lm(speed ~ pop, data=pace)
summary(linear.fit)
```

Notice that the fitted linear model is stored as an object, here
called `linear.fit`. This object contains a lot, but we'll generally
access these contents with other methods, like `summary()`.

What do the coefficient estimates mean?

There are a lot of diagnostics you can do on linear models quite
distinct from cross-validation. In this simple case, you can plot the
fit:

```{r}
plot(speed ~ pop, data=pace)
abline(linear.fit, col="red")
```

The residuals then are what is left after subtracting out the line:

```{r}
plot(speed - predict(linear.fit) ~ pop, data=pace)
```

With more dimensions, you can't easily plot these "lines". Here's one
approach:

```{r}
plot(linear.fit, 1)
```

In the presence of troublesome outliers one might consider robust
regression. This is another way of fitting linear models, different
from plain OLS. See, for example, `rlm()` from the `MASS package.
UCLA's IDRE has a [walkthrough][].

[walkthrough]: http://www.ats.ucla.edu/stat/r/dae/rreg.htm

An important thing we can do with our fitted model is make predictions
for new data.

```{r}
predict(linear.fit, data.frame(pop=200))
```

We'll return to and improve on this model shortly.


### Linear models, correlation, and R-squared

Recall the R-squared reported for our model, and notice the following.

```{r}
summary(linear.fit)$r.squared
with(pace, cor(pop, speed))^2
cor(pace$speed, predict(linear.fit))^2
1-var(residuals(linear.fit))/var(pace$speed)
```

Only the last one is the general definition of R-squared for models in
the "percent of variance explained" sense. Using OLS regression
produces the equality of them all. This post goes through the math:
[Correlation and R-Squared][].

[Correlation and R-Squared]: http://www.win-vector.com/blog/2011/11/correlation-and-r-squared/

It's often not a bad idea to explore your data for linear
relationships. This can be done graphically and numerically. For
example, the `iris` dataset:

```{r}
plot(iris, pch=19, col=iris$Species)
round(cor(iris[, 1:4])^2, 2)
```


### Formulas and design matrices

The main linear regression function in R is the `lm` function. This
function usually takes a _formula_ specifying the regression to run
and a data frame containing the data.

A formula in `R` allows you to specify a functional relationship
between variables. In R a formula is generally specified with text,
but it is itself an object.

```{r}
my.formula <- Y ~ X1 + X2 + X3
print(my.formula)
class(my.formula)
str(my.formula)
```

`R` automatically assumes there is an intercept term. You can make
this explicit by adding it as follows:

```{r, eval=FALSE}
Y ~ 1 + X1 + X2 + X3
```

You can also "remove" the intercept term (setting it to zero) in
either of the following ways:

```{r, eval=FALSE}
Y ~  0 + X1 + X2 + X3
Y ~ -1 + X1 + X2 + X3
```

As you can see, `+` and `-` don't act as addition and subtraction
operators but instead add and remove terms from the formula. There are
other operators that lose their algebraic meaning in a formula.  `:`
adds the _interaction_ of two variables. `*` adds the original terms
as well as their interaction. The following are equivalent:

```{r, eval=FALSE}
Y ~ X1 * X2 + X3
Y ~ X1 + X2 + X1:X2 + X3
```

The dot ("`.`") is sometimes useful in formulas. It stands in for all
variables in the data frame that have not been otherwise specified.
This and more is explained in `?formula`.

These formulas are very convenient for writing human-readable model
specifications. But linear regression is fundamentally just going to
work on numeric matrices. So what's going on? Internally, `lm` (and
many other `R` functions) create a _design matrix_ using
`model.matrix()` (or similar). Recalling our simple fit from earlier
we can see what this does:

```{r}
head(model.matrix(speed ~ pop, data=pace))
```

Note that the response variable (the label) is not present. Another
important thing that `model.matrix` does is making "dummies" for
categorical data:

```{r}
head(model.matrix(Sepal.Width ~ ., data=iris))
```

You can get another view into how `R` treats categorical data as
follows:

```{r}
contrasts(iris$Species)
```

_Important note_: You'll generally need exactly the same columns in
your train and test design matrices. Be careful when building a design
matrix depends on the data it happens to see!


### Linear regression as conditional mean

 * What is the mean of `Petal.Length` for each `iris` `Species`?
 * What are the coefficients of a regression of `Petal.Length` on
   `Species`?
 * What's the connection?


### Polynomial regression

We saw that `speed` curves up as `pop` increases, so we might want to
try a quadratic linear model. How can we do this?

```{r}
head(model.matrix(speed ~ pop^2, data=pace))
```

That doesn't work, but we could force it to do what we intend:

```{r}
head(model.matrix(speed ~ pop + I(pop^2), data=pace))
summary(lm(speed ~ pop + I(pop^2), data=pace))
```

This works, and R-squared is a little higher than before, but none of
the coefficients are significant. A feature and its square are
correlated, in general, and this is not good for fitting our model. In
the case of identical features, `lm` will have real problems:

```{r}
summary(lm(rnorm(100) ~ cbind(1:100, 1:100)))
```

The `poly()` function will address the correlations between powers of
a feature.

```{r}
round(cor(model.matrix(speed ~ 0 + pop + I(pop^2), data=pace)), 2)
round(cor(model.matrix(speed ~ 0 + poly(pop, 2), data=pace)), 2)
head(model.matrix(speed ~ poly(pop, 2), data=pace))
summary(lm(speed ~ poly(pop, 2), data=pace))
```

De-correlating the related features like this is nice in some ways,
but could be troublesome for predicting on new data. If we aren't
concerned about standard errors, it might be better not to do. Note
that the R-squared is the same, and the predictions are also the same.
Of course in the present case the quadratic model is still pretty
weak.


### Linear models with transformations

This scatterplot shows the relationship in the `pace` data after a
log-log transformation based on this (and the previous) plot, we
should expect the transformed data to produce a better linear fit.

```{r}
ggplot(pace, aes(y=log(speed), x=log(pop))) + geom_point()
```

Why is this true? Because the nonlinear relationship we saw before is
an example of a "power law", i.e.,	$y = x ^ b$. The log-log
transformation maps this nonlinear relationship to a linear
relationship, effectively transforming the complicated problem to a
simpler problem.

$$y = x ^ b$$
$$log(y) = log(x^b)$$
$$log(y) = b \cdot log(x)$$
$$y' = b \cdot x'$$

This is a linear fit on the transformed variables... Note that
R-squared has nearly doubled.

```{r}
log.fit <- lm(log(speed) ~ log(pop), data=pace)
summary(log.fit)
```

This kind of manuever, mapping a nonlinear problem to a linear
problem, and feature transformation generally, is often useful. We
hope that our residuals will look nicer and the model will be more
predictive. Think about what will need to be done to make predictions
with this model.

Note that we still haven't done any cross-validation!
